
---
title: "36-460/660 Final Project Report"
author: "Alex Cheng, Melody Wang, Liz Chu, Kevin Ren"
date: "April 23rd, 2024"
output:
  pdf_document:
    toc: yes
  html_document:
    code_folding: show
    toc: yes
    toc_float: yes
editor_options:
  chunk_output_type: inline
---
# Run, Run, then Run Again
```{r setup, include=FALSE}
library(knitr)
# Set knitr options for knitting code into the report:
# - Save results so that code blocks aren't re-run unless code changes (cache),
# _or_ a relevant earlier code block changed (autodep), but don't re-run if the
# only thing that changed was the comments (cache.comments)
# - Don't clutter R output with messages or warnings (message, warning)
  # This _will_ leave error messages showing up in the knitted report
# - Center figures by default
# - Don't show code by default (echo)
# - If we do show the code, tidy it up and hide the comments (tidy, tidy.opts)
opts_chunk$set(cache=TRUE, autodep=TRUE, cache.comments=FALSE,
               message=FALSE, warning=FALSE, fig.align="center",
			   echo=FALSE,
			   tidy=TRUE, tidy.opts=list(comment=FALSE))
library("nflfastR")
library(espnscrapeR)
library("ggplot2")
library(tidyverse)
library(dplyr)
library(lme4)
library(mgcv)
library(nnet)
library(pander)
df <- read_csv("runrunrun.csv")
df$outcome = ifelse(df$play_type == "run", 1, 0) 
# 1 if run, 0 otherwise (pass)
success_df = df %>% filter(success == 1)
```

```{r models}

model1 <- glm(outcome ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                as.factor(prev_play_3) + ydstogo + score_differential + 
                pass_rank + rush_rank, data=success_df, family="binomial")
model2 <- gam(outcome ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                as.factor(prev_play_3) + s(ydstogo) + s(score_differential) + 
                pass_rank + rush_rank, data=success_df, family="binomial")
model4 <- glmer(outcome ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                  as.factor(prev_play_3) + (1|posteam) + (1|defteam) + 
                  (1|posteam:defteam) + ydstogo + pass_rank, 
                data = success_df, family = "binomial")
# filtering for model 3
filt_df = success_df %>% filter((play_type == "pass" & !is.na(pass_length) & 
                      !is.na(pass_location)) | 
                     (play_type == "run" & !is.na(run_location)))
filt_df$multi = ifelse(filt_df$play_type == "run", paste("run", filt_df$run_location), 
                  paste("pass", filt_df$pass_length, filt_df$pass_location))

model3 <- multinom(multi ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                     as.factor(prev_play_3) + ydstogo + score_differential + 
                     pass_rank + rush_rank, data = filt_df, maxit = 300, 
                   trace = FALSE)

# use success_df for only successful plays, 
# filt_df for successful plays specifically for model 3
```

# Introduction

The problem of playcalling is among the greatest challenges in sports coaching, particularly in the sport of American football. Particularly poor calls -- e.g. the Seahawk's decision to pass rather than run with 1 yard to go in the closing seconds of Super Bowl XLIX -- represent snap decisions that can cause franchises and players alike to lose critical games and ultimately millions of dollars. As such, in order to avoid such outcomes coaches must be very smart about what kinds of plays to run in different scenarios, depending on the previous plays that they have called along with the context of the game. In this project we hope to answer the age-old question of what plays a football coach should call, given the types of the previous three plays in the drive, along with contextual information about the field position such as yards-to-go until a touchdown, the strength of the offensive team in terms of their passing and rushing abilities, and the difference in score in the game. We ultimately find that our fitted models on the given covariates perform strongly on both in-distribution and out-of-distribution data on their respective predictive tasks, compared to a naive baseline model.

# Data

We obtained our data from the R packages `nflreadr` and `espnscrapeR`. The main feature of the `nflreadr` package that we used in this project was its wealth in NFL play-by-play data, which would give us detail on what was happening at each step in a NFL game. We primarily used play-by-play data from the most recent 2023 NFL season for our analysis, and while the dataset featured 372 features for each play, wed only used a subset of what we thought were the relevant features to our research question, such as yard to go and point differential. The primary purpose of using `espnscrapeR` was to obtain the rushing and passing rankings of each team, which we determined by using the total number of rushing and passing yards each team ahd at the end of the 2023 seasson.

With these two datasets, we created our own dataset through four major pre-processing steps.

(1) In `nflreadr`, we would redefine a successful play as having a positive Expected Points Added (EPA) for the first and second down, and covering the yards needed to achieve a first down for the third and fourth down. 

(2) With the dataset from `nflreadr`, we would take the $k$ previous rows before a play and parse out the play type. We would then augment the dataset by adding $k$ columns, with the $i$th added column corresponding to the $i$th previous play.

(3) We would do a left-join operation with the `espnscrapeR` dataset - for the team with the possession, we would add its pass and rush ranking derived from the `espnscrapeR` dataset, resulting in two more additional columns to our dataset.

(4) We would filter out all plays that were not successes due to difficulties with counterfactuals, as well as all plays that weren't a rush or a pass. Thus, our dataset would only contain successful rush or pass plays.

This would result in the main dataset that we would work with training and building our models.

However, it would be unwise to blindly fit regressions without taking a look at possible patterns in our data, and our exploratory data analysis revealed some things that were intuitive, and some that were interesting. 

```{r}
nfl_df <- nflfastR::load_pbp()
eda_df = clean_pbp(nfl_df)
# CLEAN DATA
# remove all plays that weren't rushes or passes (this means punts, kickoffs, special teams plays, aborted plays, qb spikes, etc.)
eda_df$success = ifelse(eda_df$down >= 3 & eda_df$ydstogo > eda_df$yards_gained, 0, eda_df$success)
cleandf = eda_df %>% 
  filter(play_type == "run" | play_type == "pass") %>% 
  filter(penalty == 0) %>% 
  filter(success == 1)
# remove all plays where a penalty occured - we only want normal plays
# set third downs that didnt get a first down as unsuccessful
# 3 1/2 we can think of other successes but this is the easiest one so far
# remove all unsuccessful plays
```

The first thing we took a look at was the relationship between pass ranking and number of pass attempts, as well as rush ranking and number of rush attempts, based on data from the `espnscrapeR` dataset. At a glance, these scatterplots seem pretty intuitive - the higher the ranking for a particular aspect of the game, the more attempts that a team would have with it. This is particulary true for rushing, where the scatterplot seems extremely linear, and from a qualitative perspective also makes sense. Teams like the Ravens, 49ers, and Dolphins have fast runners in Lamar Jackson, Christian McCaffrey, and Raheem Mostert, and it would make sense to just let these players do the work in creating their own yardage. Things get a bit more interesting when you start looking at the relationship between pass attempts and pass rankings. Though this scatterplot is still relatively linear, it is significantly more scattered than the rushing plot, which may indicate that the quality of your passing offense doesn't dictate how much you actually pass as much as rushing would. What's particularly interesting is that the top rushing teams we just mentioned - Ravens, 49ers, and Dolphins - all seem to have lower pass attempts than expected, even though the rankings for the Dolphins and 49ers are actually quite high. The 49ers and the Dolphins to have plenty of lethal receiver options (Brandon Aiyuk, Tyreek Hill), but it seems like teams just prefer to rush if they're good at it, regardless of their passing abilities, which may be attributed to ball safety in these two types of plays.

```{r, rankvattempt}
rush_data = scrape_team_stats_nfl(season = 2023, stats = "rushing", role = "offense")
rush_data = rush_data %>%
  arrange(rush_yds, desc=TRUE) %>%
  mutate(rush_rank = floor(rank(-rush_yds)))
rush_data = rush_data %>% select(-c("rush_yds"))
# rush rankings = highest rush yards
pass_data = scrape_team_stats_nfl(season = 2023, stats = "passing", role = "offense")
pass_data = pass_data %>% 
  arrange(pass_yds, desc=TRUE) %>% 
  mutate(pass_rank = floor(rank(-pass_yds)))
pass_data = pass_data %>% select(-c("pass_yds"))
# pass rankings = highest pass yards
ggplot(data=rush_data, aes(x=rush_rank, y=rush_att, label = team)) + geom_point(color='blue') + labs(
  title="Rush Rank vs. Rush Attempts", 
  x="Rush Rank", 
  y="Rush Attempts Made"
) + geom_text(hjust = 0, vjust = 0)
ggplot(data=pass_data, aes(x=pass_rank, y=pass_att, label = team)) + geom_point(color='red') + labs(
  title="Pass Rank vs. Pass Attempts", 
  x="Pass Rank", 
  y="Pass Attempts Made"
) + geom_text(hjust = 0, vjust = 0)
```

We then took a look at the types of plays that were occcuring at each down. Recall that the goal of each down is to either obtain the first down or to score the ball, and after the fourth down, teams are required to turn the ball over to the opponent. Thus, from first to third down, it makes sense that the number of pass plays increase significantly, as teams are more desperate to cover the amount of yards needed to get to the first down. Fourth down is special in that there is some selection bias in these plays - NFL teams only really go for fourth down when (1) not doing so would essentially mean the forfeiture of the game, or (2) the number of yards is small enough that the team is willing to risk turning the ball over to the other team for a chance to extend the play. Nevertheless, it is still interesting that teams are more inclined to pass rather than rush on fourth down. Since yards needed is usually small, it would make intuitive sense that teams would actually prefer to *rush*, so this may be a part of NFL play calling that depends on the previous plays, which is something we want to explore.

```{r, marginal}
frequencies = nfl_df %>% filter(is.na(down) == FALSE) %>% group_by(down) %>% summarize(freq = sum(pass_attempt, na.rm = TRUE)/(sum(pass_attempt, na.rm = TRUE) + sum(rush_attempt, na.rm = TRUE)))
frequencies = data.frame(down = 1:4, "Pass Frequency" = frequencies$freq, "Run Frequency" = 1 - frequencies$freq) %>% pivot_longer(-down)
ggplot(data = frequencies, aes(x = factor(down), y = value, fill = name)) + labs(x = "Down", y = "Play Frequency", fill = "Type", title = "Play Frequency By Down") + geom_bar(position = "dodge", stat = "identity")
```

Lastly, we took a look at the relationship between the number of yards to go and the pass/rush attempts at each point. What we see is that under 3 yards the chance of a rush or a pass is generally a coin toss, but as the number of yards increases up to 30, teams are much more likely to pass rather than rush, due to the fact that they are desperate to cover more yards in their plays. Past 30 yards however, and it seems like it a coin toss once again, since the number of yards needed is so high that teams may either opt to make safer plays that won't result in turnovers (rushes), or still try to cover the amount of yards needed and extend the drive (passes).

```{r}
yardsvtest = nfl_df %>% filter(is.na(ydstogo) == FALSE) %>% group_by(ydstogo) %>% summarize(freq = sum(pass_attempt, na.rm = TRUE)/(sum(pass_attempt, na.rm = TRUE) + sum(rush_attempt, na.rm = TRUE)))
yardsvtest = data.frame(ydstogo = yardsvtest$ydstogo, "Pass Frequency" = yardsvtest$freq, "Run Frequency" = 1 - yardsvtest$freq)
legend_colors <- c("Pass" = "red", "Run" = "blue")
ggplot(data = yardsvtest, aes(x = ydstogo)) + 
  geom_line(aes(y = Pass.Frequency, color = "Pass")) + 
  geom_line(aes(y = Run.Frequency, color = "Run")) + 
  labs(title="How does distance to 1st down impact play type?", x="Distance from 1st down (yards)", y="Frequency of Play") + 
  scale_color_manual(values = legend_colors) + 
  theme_bw()
```

# Methods


To link our data to our curiosity of playcalling strategy, we focused on 4 methods: Logistic Regression, Generalized Additive Modeling, Multimonial Modeling, and Multilevel Modeling. We will use misclassification rate to compare and evaluate the models, and we will use 5-fold cross validation to account for uncertainty. 

## Logistic Regression 

As we assume a linear relationship among our covariates, a logistic regression was a relatively straightforward choice given that the nature of our question involves classifying the next best play, which is a classification problem. 
```{r}
model1 <- glm(outcome ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                as.factor(prev_play_3) + ydstogo + score_differential + 
                pass_rank + rush_rank, data=success_df, family="binomial")
summary(model1)
pander(summary(model1))
```


While all of our coefficients estimates are significant, it seems like the play types of the 3 previous plays brought the most influence on outcome in terms of magnitude. In specific, the 3rd previous play yielded the most change in log-odds of a successful game outcome -  the change in the game outcome when the 3rd previous play was a pass compared to when it was the base condition (first_play) was 0.3159 while it was 0.5934 for a run. Since the log odds for the 1st and 2nd previous play was negative, we convert them to an exponential scale and discover that for the 1st previous play, a pass had a higher odds ratio of 0.7452765 than that of the a run. On the second play, however, a pass produced a lower odds ratio of 0.5797258 than the 0.6693153 yielded by a run. Interestingly, it seems like the odds ratio for both types of plays does not necessarily decrease or increase as k changes. More so, yards-to-go, score differential, play_rank and pass_rank seem to have more of an influence on outcome than the previous 2nd and 3rd plays. This may signify that we need to work with more intricate relationships and compare these results with the results of more complex models. 


```{r}
pander(exp(coef(model1))[1:7])
```

```{r}
unique(success_df$prev_play_1)
exp(0.3159)
exp(0.5934)

exp(-0.294 )
exp(-0.3718)

exp(-0.5452)
exp(-0.4015)

exp(-0.1193)
exp(0.01975)
exp(0.01432)
exp(-0.01443)
```
## General Additive Model 


Our logistic regression developed promising results, but did not necessarily account for the possibility that we had a non-linear relationship between our predictors and game outcome. By allowing us to smooth over predictors that may have non-linear relationships, we can take advantage of general additive models to flexibly model these intricate relationships. 

```{r}
model2 <- gam(outcome ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                as.factor(prev_play_3) + s(ydstogo) + s(score_differential) + 
             pass_rank +rush_rank, data=success_df, family="binomial")
summary(model2)
```


Odds Ratio: 
```{r}
pander(exp(coef(model2))[1:7])
```

```{r}
#Odds Ratio 
pander(exp(c(Intercept= 0.194516, prev_1_pass = -0.086438, prev_1_run = -0.067655, prev_2_pass = -0.649021, prev_2_run = -0.501338, prev_3_run = 0.326274, prev_3_pass = 0.596848, pass_rank = 0.014828, push_rank = -0.014406)))
```
Surprisingly, after smoothing for score_differential and yds_to_go, the previous 1st play type lost its significance while the ratio for both play types increased substantially. Additionally, the run had a higher odds ratio than that of a pass for the first previous play - the exact opposite as our previous model. However, we developed similar results as logistic regression for the previous second and third play types. We hypothesize that the significant change in the first previous play stems from ydstogo and score differential having a sizable interaction effect on the first previous play, which makes sense as ydstogo and score differential measure current conditions of the game, and the most recent previous play contributes more change to current situations than further plays in time.

## Multimonial Regression

We proceed in experimenting with model performance with multimonial modeling, which allows the model to perform with more granularity while still maintaining the form of logistic regression. In specific, we have the presence of short/deep and left/middle/right passes and left/middle/right runs, so a multimonial model would be suitable for interpreting the effect on predictors of multiple categories on the outcome.We first filter out the NAs in the pass types, as there are more null values in our more granular predictors. 

```{r}
filt_df = success_df %>% filter((play_type == "pass" & !is.na(pass_length) & 
                      !is.na(pass_location)) | 
                     (play_type == "run" & !is.na(run_location)))
filt_df$multi = ifelse(filt_df$play_type == "run", paste("run", filt_df$run_location), 
                  paste("pass", filt_df$pass_length, filt_df$pass_location))

model3 <- multinom(multi ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                     as.factor(prev_play_3) + ydstogo + score_differential + 
                     pass_rank + rush_rank, data = filt_df, maxit = 300, 
                   trace = FALSE)
```

```{r}
pander(exp(coef(summary(model3))))
```

Pass short middle was the best-outcome strategy in term of odds ratio for all the previous plays with the exception of the 3rd previous play. We do not see a lot of difference in Run directions for the previous plays or for the other predictors, but do note that a middle run has a lower log odds than other directions for score_differential and ydstogo. This is reasonable - while a middle run may bring you closer to the other side of the field, the linebackers and nose tackles on defense often clog up the middle lane, which minimizes score potential. Middle runs also have lower odds ratio more often than not in previous plays. Analysis from other football analytics gurus have alsos shown that [short passes to the middle generate the most EPA]{https://sumersports.com/the-zone/hitting-the-hard-shots-why-the-middle-of-the-field-is-the-most-effective-throw-in-football-despite-the-best-quarterbacks-succeeding-elsewhere/}, but are just generally difficult to execute due to the amount of time the ball is in the QBs hands to generate such a play, as well as defensive schemes often wandering near the middle.



## Multilevel Regression

Since our effects may not be constant, we let go of the fixed effects assumption present in the previous models to create a multilevel model. Specifically, we take advantage of the model's suitability for nested structures in accounting for dependencies among the same cluster. We also included random intercepts for the offensive and defensive teams and their interaction with each other. The idea behind this model is not only include the differences among each offensive team, but also the influences of which team is on the defense and how they interact with the the offensive team. For example, the Dolphins and the Jets have very different run tendencies, and would also exhibit different play-calling behaviors when playing the Browns versus the Seahawks. 

```{r}
model4 <- glmer(outcome ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                  as.factor(prev_play_3) + (1|posteam) + (1|defteam) + 
                  (1|posteam:defteam) + ydstogo + pass_rank, 
                data = success_df, family = "binomial")
```
```{r}
summary(model4)
```

```{r}
pander(coef(summary(model4)))
```
```{r}
pander(data.frame(Term = c("posteam:defteam", "defteam", "posteam"), Variance = c(0.05676, 0.01376, 0.01819)))
```

The multilevel model generally follows that of logistic regression in that the 3rd previous play had the highest effect and the 2nd previous play had the lowest. However, we do find that the log odds seem to increase slightly for the first previous play and decrease in magnitude for the 3rd one.
In terms of random effects, the interaction between offensive and defensive teams had a variance of 0.05676, which suggests that there was moderate variability on the intercepts between different combinations of offensive and defensive teams. There seemed to be slightly more variability between different offensive teams than in different defensive teams. On the other hand, correlation between fixed effects seems to be more prominent in prev_plays and yards to go than the different k previous plays with each other. 


In all its entirety, our four model each brought us varying insights on the underlying relationship between our data and our goal at predicting the best play given the k previous plays. We now aim to test the validity of each of these models. 


# Results


*Describes your results. This can include tables and plots showing your results, as well as text describing how your models worked and the appropriate interpretations of the relevant output. I do not want to you to write out the textbook interpretations of all model coefficients! I only want you to interpret the output that is relevant for your question of interest that is framed in the introduction.*

```{r model results}

cv.misclass <- function(data, nfolds = 5, model = "none") {
  # model will be either glm, gam, glmer, or multinom
  n <- nrow(data)
  fold.labels <- sample(rep(1:nfolds, length.out = n))
  misclass <- matrix(NA, nrow = nfolds, ncol = 1)
  for (fold in 1:nfolds) {
    test.rows <- which(fold.labels == fold)
    train <- data[-test.rows, ]
    test <- data[test.rows, ]
    if (model == "glm") {
      current.model <- glm(outcome ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                             as.factor(prev_play_3) + ydstogo + score_differential + 
                             pass_rank + rush_rank, data = train, family = "binomial")
    } else if (model == "gam") {
      current.model <- gam(outcome ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                             as.factor(prev_play_3) + s(ydstogo) + s(score_differential) + 
                             pass_rank + rush_rank, data = train, family = "binomial")
    } else if (model == "glmer") {
      current.model <- glmer(outcome ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                               as.factor(prev_play_3) + (1|posteam) + (1|defteam) + 
                               (1|posteam:defteam) + ydstogo + pass_rank, 
                             data = train, family = "binomial")
    } else {
      current.model <- multinom(multi ~ as.factor(prev_play_1) + as.factor(prev_play_2) + 
                                  as.factor(prev_play_3) + ydstogo + score_differential + 
                                  pass_rank + rush_rank, data = train, maxit = 300, 
                                trace = FALSE)
    }
    if (model == "multinom") {
      predictions <- predict(current.model, newdata = test, type="class")
      test.responses = test$multi
    } else {
      predictions <- round(predict(current.model, newdata = test, type="response"))
      test.responses = test$outcome
    }
    total_wrong <- sum(predictions != test.responses)
    total <- length(predictions)
    
    misclass[fold, 1] <- total_wrong / total
  }
  return(colMeans(misclass))
}

model1.misclass = cv.misclass(success_df, model = "glm")
model2.misclass = cv.misclass(success_df, model = "gam")
model3.misclass = cv.misclass(filt_df, model = "multinom")
model4.misclass = cv.misclass(success_df, model = "glmer")

```

```{r displaying misclassification rates}
output = data.frame(model1.misclass, model2.misclass, model3.misclass, model4.misclass)
colnames(output) = c("Model 1", "Model 2", "Model 3", "Model 4")
rownames(output) = c("Misclassification Rate")
knitr::kable(t(output), digits=4)
```

With the four models we trained, we decided to evaluate them using misclassification rate, given that our problem is that of classification (labeling each play as either a run or a pass for three of our models, and classifying each play with its length and location for the multinomial model). In order to account for the uncertainty of our models' performance on out-of-sample data, we performed 5-fold cross-validation on each of our models and achieved average misclassification rates, shown above. Our second model, the GAM, performs the best with a 5-fold CV misclassification rate of 0.35, meaning that on average, we expect our GAM model to incorrectly predict the run/pass about 35\% of the time. Our first model and last model (GLM and GLMER models) perform very similarly, but ever so slightly worse (predicting incorrectly about 36\% of the time). Our third model, the multinomial model, performs the worst with a misclassification rate of about 77\%. This is likely due to the fact that our classification is more granular, as we now have nine different classes we could predict instead of the two we previously had. 



# Discussion

Overall it was very interesting to see how similarly our chosen models performed given the breadth of the features we chose. This may suggest that our features were not distinct enough from each other or we did not select enough features. This could be plausible as play-calling is a clearly complex process representing the battle between offensive and defensive coaching minds along with the players on the field, not to mention the field position of the play itself. As such, it is plausible that one limitation of our approach is simply that we did not perform enough vetting of the features we chose. In the future we could potentially add more features such as the ranking of the defensive team on the field versus the pass or the run (currently no defensive statistics are considered, which could be troublesome) as well as including features such as which team is home versus away, or perhaps tuning our models on different number of prior plays that the model is able to see.

Secondly, the question of what our training data should look like was something that we could potentially change in the future as well. By nature our data fails to reveal counterfactual outcomes, e.g. the offensive team induces a treatment in the form of a play call, causing some outcome in the form of yards gained, which doesn't allow for seeing what *would* have happened should a different play have been run. As a result we chose not to include what we determined to be 'failed' plays in our training process, and so we modeled the probability that a given play occurred given that it was successful. This process therefore wastes lots of data: no information from the failed plays is reflected in our trained models. Expanding our analysis to be a causal inference question in which we attempt to do some modeling on what play calls could potentially reverse or alter the outcomes of failed plays could represent an interesting statistical question under which future research could be conducted.



