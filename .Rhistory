resampling = function() { return(resample.data.frame(df))}
model.1.ci = bootstrap.ci(statistic = model.1.estimator,
simulator = resampling,
t.hat = model.1.pred, B = 100, level = 0.95)
# uncertainty: bootstrapping! yay!
model.1.estimator = function(data) {
mdl = npreg(log(pcgmp) ~ log(pop), data = data)
return(exp(predict(mdl, newdata=pitt_pop)) - exp(predict(mdl, newdata=pitt)))
}
model.2.estimator = function(data) {
mdl = npreg(log(pcgmp) ~ log(ict) + log(pop), data = data)
return(exp(predict(mdl, newdata=pitt_ict)) - exp(predict(mdl, newdata=pitt)))
}
model.3.estimator = function(data) {
mdl = npreg(log(pcgmp) ~ log(ict), data = data)
return(exp(predict(mdl, newdata=pitt_ict)) - exp(predict(mdl, newdata=pitt)))
}
resampling = function() { return(resample.data.frame(df))}
model.1.ci = bootstrap.ci(statistic = model.1.estimator,
simulator = resampling,
t.hat = model.1.pred, B = 100, level = 0.95)
model.2.ci = bootstrap.ci(statistic = model.2.estimator,
simulator = resampling,
t.hat = model.2.pred, B = 100, level = 0.95)
model.3.ci = bootstrap.ci(statistic = model.3.estimator,
simulator = resampling,
t.hat = model.3.pred, B = 100, level = 0.95)
model.1.ci
model.2.ci
model.3.ci
model.1.pred
model.2.pred
model.3.pred
help(npreg)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
knitr::opts_chunk$set(fig.pos = 'H')
library(ggplot2)
library(dplyr)
library(knitr)
library(gamair)
library(tinytex)
library(np)
library(mgcv)
library(cowplot)
options(np.messages = FALSE)
source("http://www.stat.cmu.edu/~cshalizi/uADA/24/lectures/ch06-no-demos.R")
df = read.csv("gmp-2006.csv")
pcgmp_plot = ggplot(data=df, aes(x=pcgmp)) + geom_histogram(bins=25, fill="lightgreen") + labs(
title="Marginal Distribution of Per Capita GMP",
x="Per Capita GMP ($)",
y="Frequency"
)
pop_plot = ggplot(data=df, aes(x=pop / 1000)) + geom_histogram(bins=25, fill="steelblue") + labs(
title="Marginal Distribution of Population",
x="Population (thousands of people)",
y="Frequency"
)
plot_grid(pcgmp_plot, pop_plot)
finance_plot = ggplot(data=df, aes(x=finance)) + geom_histogram(bins=25, fill="gold") + labs(
title="Marginal Distribution of Proportion \nEconomy in Finance",
x="Proportion",
y="Frequency"
)
prof.tech_plot = ggplot(data=df, aes(x=prof.tech)) + geom_histogram(bins=25, fill="lightsalmon") + labs(
title="Marginal Distribution of Proportion \nEconomy in Prof & Tech",
x="Proportion",
y="Frequency"
)
ict_plot = ggplot(data=df, aes(x=ict)) + geom_histogram(bins=25, fill="violetred") + labs(
title="Marginal Distribution of Proportion \nEconomy in IC Tech",
x="Proportion",
y="Frequency"
)
management_plot = ggplot(data=df, aes(x=management)) + geom_histogram(bins=25, fill="lightslateblue") + labs(
title="Marginal Distribution of Proportion \nEconomy in Management",
x="Proportion",
y="Frequency"
)
plot_grid(finance_plot, prof.tech_plot, ict_plot, management_plot, nrow=2)
pop_pcgmp_plot = ggplot(data=df, aes(x=pop / 1000, y=pcgmp)) + geom_point(color="turquoise3") + labs(
title="Distribution of Population vs. \nPer Capita GMP",
x="Population (thousands of people)",
y="Per Capita GMP ($)"
)
ict_pcgmp_plot = ggplot(data=df, aes(x=finance, y=pcgmp)) + geom_point(color="brown") + labs(
title="Distribution of Proportion Economy in \nIC Tech vs. Per Capita GMP",
x="Proportion",
y="Per Capita GMP ($)"
)
plot_grid(pop_pcgmp_plot, ict_pcgmp_plot)
knitr::include_graphics("402dag1.png")
knitr::include_graphics("402dag2.png")
knitr::include_graphics("402dag3.png")
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - pitt$pcgmp
# uncertainty: bootstrapping! yay!
model.1.estimator = function(data) {
mdl = npreg(log(pcgmp) ~ log(pop), data = data)
return(exp(predict(mdl, newdata=pitt_pop)) - pitt$pcgmp)
}
model.2.estimator = function(data) {
mdl = npreg(log(pcgmp) ~ log(ict) + log(pop), data = data)
return(exp(predict(mdl, newdata=pitt_ict)) - pitt$pcgmp)
}
model.3.estimator = function(data) {
mdl = npreg(log(pcgmp) ~ log(ict), data = data)
return(exp(predict(mdl, newdata=pitt_ict)) - pitt$pcgmp)
}
resampling = function() { return(resample.data.frame(df))}
model.1.ci = bootstrap.ci(statistic = model.1.estimator,
simulator = resampling,
t.hat = model.1.pred, B = 100, level = 0.95)
model.2.ci = bootstrap.ci(statistic = model.2.estimator,
simulator = resampling,
t.hat = model.2.pred, B = 100, level = 0.95)
model.3.ci = bootstrap.ci(statistic = model.3.estimator,
simulator = resampling,
t.hat = model.3.pred, B = 100, level = 0.95)
model.1.ci
model.2.ci
model.3.ci
model.1.pred
model.2.pred
model.3.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred
exp(predict(model.3, newdata=pitt_ict))
pitt$pcgmp
pitt_ict
pitt
model.3.pred
model.3.ci
model.3.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ (ict), data = df) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg((pcgmp) ~ (ict), data = df) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg((pcgmp) ~ (ict), data = df) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = (predict(model.3, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred
help(npreg)
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df, tol=0.001, ftol=0.0001) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df, tol=0.001, ftol=0.001) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - pitt$pcgmp
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df, tol=0.001, ftol=0.001) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df, tol=0.01, ftol=0.01) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df, tol=0.01, ftol=0.01) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - pitt$pcgmp
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - pitt$pcgmp
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - exp(predict(model.3, newdata=pitt))
model.3.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df, tol=0.01, ftol=0.01) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - exp(predict(model.1, newdata=pitt))
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - exp(predict(model.2, newdata=pitt))
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - exp(predict(model.3, newdata=pitt))
model.3.pred
model.1.pred
model.2.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df, tol=0.01, ftol=0.01) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df, tol=0.01, ftol=0.01) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df, tol=0.01, ftol=0.01) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - exp(predict(model.1, newdata=pitt))
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - exp(predict(model.2, newdata=pitt))
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - exp(predict(model.3, newdata=pitt))
model.3.pred
model.1 = npreg(log(pcgmp) ~ log(pop), data = df, tol=0.01, ftol=0.01) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df, tol=0.01, ftol=0.01) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df, tol=0.01, ftol=0.01) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - exp(predict(model.1, newdata=pitt))
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - exp(predict(model.2, newdata=pitt))
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - exp(predict(model.3, newdata=pitt))
model.1 = npreg(log(pcgmp) ~ log(pop), data = df, tol=0.01, ftol=0.01) # theory 1 pop, theory 2 pop
model.2 = npreg(log(pcgmp) ~ log(ict) + log(pop), data = df, tol=0.01, ftol=0.01) # theory 2 ict
model.3 = npreg(log(pcgmp) ~ log(ict), data = df, tol=0.01, ftol=0.01) # theory 3 ict
# 0 # theory 1 ict, theory 3 pop
pitt = subset(df, MSA == "Pittsburgh, PA")
pitt_pop = pitt
pitt_pop$pop = pitt$pop * 2
pitt_ict = pitt
pitt_ict$ict = pitt$ict + 0.1
model.1.pred = exp(predict(model.1, newdata=pitt_pop)) - exp(predict(model.1, newdata=pitt))
model.2.pred = exp(predict(model.2, newdata=pitt_ict)) - exp(predict(model.2, newdata=pitt))
model.3.pred = exp(predict(model.3, newdata=pitt_ict)) - exp(predict(model.3, newdata=pitt))
# uncertainty: bootstrapping! yay!
model.1.estimator = function(data) {
mdl = npreg(log(pcgmp) ~ log(pop), data = data)
return(exp(predict(mdl, newdata=pitt_pop)) - pitt$pcgmp)
}
model.2.estimator = function(data) {
mdl = npreg(log(pcgmp) ~ log(ict) + log(pop), data = data)
return(exp(predict(mdl, newdata=pitt_ict)) - pitt$pcgmp)
}
model.3.estimator = function(data) {
mdl = npreg(log(pcgmp) ~ log(ict), data = data)
return(exp(predict(mdl, newdata=pitt_ict)) - pitt$pcgmp)
}
resampling = function() { return(resample.data.frame(df))}
model.1.ci = bootstrap.ci(statistic = model.1.estimator,
simulator = resampling,
t.hat = model.1.pred, B = 100, level = 0.95)
model.2.ci = bootstrap.ci(statistic = model.2.estimator,
simulator = resampling,
t.hat = model.2.pred, B = 100, level = 0.95)
model.3.ci = bootstrap.ci(statistic = model.3.estimator,
simulator = resampling,
t.hat = model.3.pred, B = 100, level = 0.95)
model.1.ci
model.2.ci
model.3.ci
model.1.pred
model.2.pred
model.2.ci
model.3.pred
model.3.ci
help(poLCA)
??poLCA
library(poLCA)
install.packages('poLCA')
head(df)
cond.model.1 = npreg(pcgmp ~ ict + pop, data=df) # check whether ict has a significant coeff
cond.model.2 = npreg(pcgmp ~ pop + finance +
prof.tech + ict + management, data=df) # check whether pop has a significant coeff
cond.model.3 = npreg(pop ~ ict + pcgmp, data=df) # check whether ict has a significant coeff
coeffs(cond.model.2)
help(coeff)
??coeff
coefficients(cond.model.2)
cond.model.2
cond.model.1 = npreg(pcgmp ~ s(ict) + s(pop), data=df) # check whether ict has a significant coeff
cond.model.1 = npreg(pcgmp ~ ict + pop, data=df) # check whether ict has a significant coeff
cond.model.2 = npreg(pcgmp ~ pop + finance +
prof.tech + ict + management, data=df) # check whether pop has a significant coeff
cond.model.3 = npreg(pop ~ ict + pcgmp, data=df) # check whether ict has a significant coeff
help("npcdens")
help(npreg)
cond.model.1 = lm(pcgmp ~ ict + pop, data=df) # check whether ict has a significant coeff
cond.model.2 = lm(pcgmp ~ pop + finance +
prof.tech + ict + management, data=df) # check whether pop has a significant coeff
cond.model.3 = lm(pop ~ ict + pcgmp, data=df) # check whether ict has a significant coeff
summary(cond.model.1)
cond.model.1.estimator = function(data) {
mdl = lm(pcgmp ~ ict + pop, data=data)
return(coefficients(mdl))
}
cond.model.2.estimator = function(data) {
mdl = lm(pcgmp ~ pop + finance +
prof.tech + ict + management, data=data)
return(coefficients(mdl))
}
cond.model.3.estimator = function(data) {
mdl = lm(pop ~ ict + pcgmp, data=data)
return(coefficients(mdl))
}
resampling = function() { return(resample.data.frame(df))}
cond.model.1.cis = bootstrap.ci(statistic = cond.model.1.estimator,
simulator = resampling,
t.hat = coefficients(cond.model.1), B = 100, level = 0.95)
cond.model.2.cis = bootstrap.ci(statistic = cond.model.2.estimator,
simulator = resampling,
t.hat = coefficients(cond.model.2), B = 100, level = 0.95)
cond.model.3.cis = bootstrap.ci(statistic = cond.model.3.estimator,
simulator = resampling,
t.hat = coefficients(cond.model.3), B = 100, level = 0.95)
cond.model.1.cis
cond.model.2.cis
cond.model.3.cis
cond.model.1 = lm(log(pcgmp) ~ log(ict) + log(pop), data=df) # check whether ict has a significant coeff
cond.model.2 = lm(pcgmp ~ pop + finance +
prof.tech + ict + management, data=df) # check whether pop has a significant coeff
cond.model.3 = lm(pop ~ ict + pcgmp, data=df) # check whether ict has a significant coeff
cond.model.1.estimator = function(data) {
mdl = lm(log(pcgmp) ~ log(ict) + log(pop), data=data)
return(coefficients(mdl))
}
cond.model.2.estimator = function(data) {
mdl = lm(pcgmp ~ pop + finance +
prof.tech + ict + management, data=data)
return(coefficients(mdl))
}
cond.model.3.estimator = function(data) {
mdl = lm(pop ~ ict + pcgmp, data=data)
return(coefficients(mdl))
}
resampling = function() { return(resample.data.frame(df))}
cond.model.1.cis = bootstrap.ci(statistic = cond.model.1.estimator,
simulator = resampling,
t.hat = coefficients(cond.model.1), B = 100, level = 0.95)
cond.model.2.cis = bootstrap.ci(statistic = cond.model.2.estimator,
simulator = resampling,
t.hat = coefficients(cond.model.2), B = 100, level = 0.95)
cond.model.3.cis = bootstrap.ci(statistic = cond.model.3.estimator,
simulator = resampling,
t.hat = coefficients(cond.model.3), B = 100, level = 0.95)
cond.model.1.cis
cond.model.1 = lm(log(pcgmp) ~ log(ict) + log(pop), data=df) # check whether ict has a significant coeff
cond.model.2 = lm(log(pcgmp) ~ log(pop) + log(finance) +
log(prof.tech) + log(ict) + log(management), data=df) # check whether pop has a significant coeff
cond.model.3 = lm(log(pop) ~ log(ict) + log(pcgmp), data=df) # check whether ict has a significant coeff
cond.model.1.estimator = function(data) {
mdl = lm(log(pcgmp) ~ log(ict) + log(pop), data=data)
return(coefficients(mdl))
}
cond.model.2.estimator = function(data) {
mdl = lm(pcgmp ~ pop + finance +
prof.tech + ict + management, data=data)
return(coefficients(mdl))
}
cond.model.3.estimator = function(data) {
mdl = lm(pop ~ ict + pcgmp, data=data)
return(coefficients(mdl))
}
resampling = function() { return(resample.data.frame(df))}
cond.model.1.cis = bootstrap.ci(statistic = cond.model.1.estimator,
simulator = resampling,
t.hat = coefficients(cond.model.1), B = 100, level = 0.95)
cond.model.2.cis = bootstrap.ci(statistic = cond.model.2.estimator,
simulator = resampling,
t.hat = coefficients(cond.model.2), B = 100, level = 0.95)
cond.model.3.cis = bootstrap.ci(statistic = cond.model.3.estimator,
simulator = resampling,
t.hat = coefficients(cond.model.3), B = 100, level = 0.95)
cond.model.1.cis
cond.model.2.cis
cond.model.3.cis
model.1
mod = npreg(log(pcgmp) ~ s(log(ict)), data=df)
npreg(pcgmp ~ s(ict))
npreg(pcgmp ~ s(ict), data=df)
npcdens(pcgmp ~ s(ict), data=df)
npcdens(pcgmp ~ ict, data=df)
model.1.cond = npreg(log(pgcmp) ~ log(ict) + log(population), data=df)
model.1.cond = npreg(log(pcgmp) ~ log(ict) + log(population), data=df)
model.1.cond = npreg(log(pcgmp) ~ log(ict) + log(pop), data=df)
model.1.uncond = npreg(log(pcgmp) ~ log(ict), data=df)
model.1.cond
model.1.cond = npcdens(log(pcgmp) ~ log(ict) + log(pop), data=df)
model.1.uncond = npcdens(log(pcgmp) ~ log(ict), data=df)
model.1.cond
model.1.cond$log_likelihood
model.1.uncond$log_likelihood
model.1.cond = npcdens((pcgmp) ~ (ict) + (pop), data=df)
model.1.cond = npcdens(pcgmp ~ ict + pop, data=df)
model.1.uncond = npcdens(pcgmp ~ ict, data=df)
model.1.cond$log_likelihood
model.1.uncond$log_likelihood
model.1.cond = npcdens(pcgmp ~ ict + pop, data=df, tol=1e-3, ftol=1e-3)
model.1.uncond = npcdens(pcgmp ~ ict, data=df, tol=1e-3, ftol=1e-3)
model.1.cond$log_likelihood
model.1.uncond$log_likelihood
help(npcden)
help(npcdens)
model.1.cond = npcdens(pcgmp ~ ict + pop, data=df, tol=1e-3, ftol=1e-3)
model.1.uncond = npcdens(pcgmp ~ ict, data=df, tol=1e-3, ftol=1e-3)
model.2.cond = npcdens(pcgmp ~ pop + management + finance + prof.tech + ict, data=df,
tol=1e-3, ftol=1e-3)
model.2.uncond = npcdens(pcgmp ~ pop, data=df,
tol=1e-3, ftol=1e-3)
model.3.cond = npcdens(pop ~ ict + pcgmp, data=df, tol=1e-3, ftol=1e-3)
model.3.uncond = npcdens(pop ~ ict, data=df, tol=1e-3, ftol=1e-3)
model.3.cond
model.3.uncond
model.1.cond.estimator = function(data) {
mdl.1 = npcdens(pcgmp ~ ict + pop, data=data, tol=1e-3, ftol=1e-3)
mdl.1.red = npcdens(pcgmp ~ ict, data=data, tol=1e-3, ftol=1e-3)
return(mdl.1$log_likelihood - mdl.1.red$log_likelihood)
}
model.2.cond.estimator = function(data) {
mdl.2 = npcdens(pcgmp ~ pop + management + finance + prof.tech + ict, data=data, tol=1e-3, ftol=1e-3)
mdl.2.red = npcdens(pcgmp ~ pop, data=data, tol=1e-3, ftol=1e-3)
return(mdl.2$log_likelihood - mdl.2.red$log_likelihood)
}
model.3.cond.estimator = function(data) {
mdl.3 = npcdens(pop ~ ict + pcgmp, data=data, tol=1e-3, ftol=1e-3)
mdl.3.red = npcdens(pop ~ ict, data=data, tol=1e-3, ftol=1e-3)
return(mdl.3$log_likelihood - mdl.3.red$log_likelihood)
}
resampling = function() { return(resample.data.frame(df))}
model.1.ci = bootstrap.ci(statistic = model.1.cond.estimator,
simulator = resampling,
t.hat = model.1.cond$log_likelihood - model.1.uncond$log_likelihood,
B = 100, level = 0.95)
model.2.ci = bootstrap.ci(statistic = model.2.cond.estimator,
simulator = resampling,
t.hat = model.2.cond$log_likelihood - model.2.uncond$log_likelihood,
B = 100, level = 0.95)
